{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.4sp5sua7cbgxueoc35yp2asoicyyeqzz.gfortran-win_amd64.dll\n",
      "c:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22524/1019278346.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mq_latent_loss\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0m_commitment_cost\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0me_latent_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m \u001B[0mquantized\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mflat_input\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mquantized\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mflat_input\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22524/1019278346.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mq_latent_loss\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0m_commitment_cost\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0me_latent_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m \u001B[0mquantized\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mflat_input\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mquantized\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mflat_input\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx\u001B[0m in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2020.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1146\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1147\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1149\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2020.2.1\\plugins\\python\\helpers\\pydev\\pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1160\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1162\u001B[1;33m                 \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0.01\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "_num_embeddings = 4 # 100\n",
    "_embedding_dim = 3 # 50\n",
    "_commitment_cost = 0.25\n",
    "\n",
    "inputs = torch.tensor([\n",
    "    [1.,2.,3.,4.,5.,6.,7.,8.,9.],\n",
    "    [4.,5.,6.,7.,8.,9.,1.,2.,3.],\n",
    "    [7.,8.,9.,1.,2.,3.,4.,5.,6.]\n",
    "])\n",
    "\n",
    "_embedding = nn.Embedding(_num_embeddings, _embedding_dim)\n",
    "_embedding.weight.data = torch.tensor([\n",
    "    [1.,2.,3.],\n",
    "    [4.,5.,6.],\n",
    "    [7.,8.,9.],\n",
    "    [5.,5.,5.]\n",
    "])\n",
    "\n",
    "\n",
    "inputs = inputs.contiguous()\n",
    "input_shape = inputs.shape\n",
    "\n",
    "flat_input = inputs.view(-1, _embedding_dim)\n",
    "\n",
    "repeated_input = flat_input.repeat(1, _embedding.weight.size(0)).view(-1, _embedding_dim)\n",
    "repeated_embedding = _embedding.weight.repeat(flat_input.size(0), 1).view(-1, _embedding_dim)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "sims = cos(repeated_input, repeated_embedding).reshape(-1, len(_embedding.weight))\n",
    "\n",
    "# Encoding\n",
    "encoding_indices = torch.argmax(sims, dim=1).unsqueeze(1)\n",
    "encodings = torch.zeros(encoding_indices.shape[0], _num_embeddings, device=inputs.device)\n",
    "encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "# Quantize and unflatten\n",
    "quantized = torch.matmul(encodings, _embedding.weight).view(flat_input.shape)\n",
    "\n",
    "# Loss\n",
    "e_latent_loss = F.cosine_embedding_loss(quantized.detach(), flat_input, torch.ones(len(quantized)))\n",
    "q_latent_loss = F.cosine_embedding_loss(quantized, flat_input.detach(), torch.ones(len(quantized)))\n",
    "\n",
    "loss = q_latent_loss + _commitment_cost * e_latent_loss\n",
    "\n",
    "quantized = flat_input + (quantized - flat_input).detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}