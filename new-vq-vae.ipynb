{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [VQ-VAE](https://arxiv.org/abs/1711.00937) by  [AÃ¤ron van den Oord](https://twitter.com/avdnoord) et al. in PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Variational Auto Encoders (VAEs) can be thought of as what all but the last layer of a neural network is doing, namely feature extraction or seperating out the data. Thus given some data we can think of using a neural network for representation generation. \n",
    "\n",
    "Recall that the goal of a generative model is to estimate the probability distribution of high dimensional data such as images, videos, audio or even text by learning the underlying structure in the data as well as the dependencies between the different elements of the data. This is very useful since we can then use this representation to generate new data with similar properties. This way we can also learn useful features from the data in an unsupervised fashion.\n",
    "\n",
    "The VQ-VAE uses a discrete latent representation mostly because many important real-world objects are discrete. For example in images we might have categories like \"Cat\", \"Car\", etc. and it might not make sense to interpolate between these categories. Discrete representations are also easier to model since each category has a single value whereas if we had a continous latent space then we will need to normalize this density function and learn the dependencies between the different variables which could be very complex.\n",
    "\n",
    "### Code\n",
    "\n",
    "I have followed the code from the TensorFlow implementation by the author which you can find here [vqvae.py](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py) and [vqvae_example.ipynb](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb). \n",
    "\n",
    "Another PyTorch implementation is found at [pytorch-vqvae](https://github.com/ritheshkumar95/pytorch-vqvae).\n",
    "\n",
    "\n",
    "## Basic Idea\n",
    "\n",
    "The overall architecture is summarized in the diagram below:\n",
    "\n",
    "![](images/vq-vae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a latent embedding space of dimension `[K, D]` where `K` are the number of embeddings and `D` is the dimensionality of each latent embeddng vector, i.e. $e_i \\in \\mathbb{R}^{D}$. The model is comprised of an encoder and a decoder. The encoder will map the input to a sequence of discrete latent variables, whereas the decoder will try to reconstruct the input from these latent sequences. \n",
    "\n",
    "More preciesly, the model will take in batches of RGB images,  say $x$, each of size 32x32 for our example, and pass it through a ConvNet encoder producing some output $E(x)$, where we make sure the channels are the same as the dimensionality of the latent embedding vectors. To calculate the discrete latent variable we find the nearest embedding vector and output it's index. \n",
    "\n",
    "The input to the decoder is the embedding vector corresponding to the index which is passed through the decoder to produce the reconstructed image. \n",
    "\n",
    "Since the nearest neighbour lookup has no real gradient in the backward pass we simply pass the gradients from the decoder to the encoder  unaltered. The intuition is that since the output representation of the encoder and the input to the decoder share the same `D` channel dimensional space, the gradients contain useful information for how the encoder has to change its output to lower the reconstruction loss.\n",
    "\n",
    "## Loss\n",
    "\n",
    "The total loss is actually composed of three components\n",
    "\n",
    "1. **reconstruction loss**: which optimizes the decoder and encoder\n",
    "1. **codebook loss**: due to the fact that gradients bypass the embedding, we use a dictionary learning algorithm  which uses an $l_2$  error to move the embedding vectors $e_i$ towards the encoder output\n",
    "1. **commitment loss**:  since the volume of the embedding space is dimensionless, it can grow arbirtarily if the embeddings $e_i$ do not train as fast as  the encoder parameters, and thus we add a commitment loss to make sure that the encoder commits to an embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create run dir and init logging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "run_dir = f'./runs/{run_id}'\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger(f'{run_id}')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "fh = logging.FileHandler(filename=f'{run_dir}/info.log', encoding='utf-8')\n",
    "fh.setLevel(logging.INFO)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "fh = logging.FileHandler(filename=f'{run_dir}/debug.log', encoding='utf-8')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.addHandler(fh)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20836/544958103.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdownloader\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mapi\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mmodel_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'glove-wiki-gigaword-50'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mwv\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mapi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\gensim\\downloader.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(name, return_path)\u001B[0m\n\u001B[0;32m    488\u001B[0m     \"\"\"\n\u001B[0;32m    489\u001B[0m     \u001B[0m_create_base_dir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 490\u001B[1;33m     \u001B[0mfile_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_filename\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    491\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfile_name\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    492\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Incorrect model/corpus name\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\gensim\\downloader.py\u001B[0m in \u001B[0;36m_get_filename\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    424\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    425\u001B[0m     \"\"\"\n\u001B[1;32m--> 426\u001B[1;33m     \u001B[0minformation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    427\u001B[0m     \u001B[0mcorpora\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minformation\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'corpora'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    428\u001B[0m     \u001B[0mmodels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minformation\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'models'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\gensim\\downloader.py\u001B[0m in \u001B[0;36minfo\u001B[1;34m(name, show_only_latest, name_only)\u001B[0m\n\u001B[0;32m    266\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    267\u001B[0m     \"\"\"\n\u001B[1;32m--> 268\u001B[1;33m     \u001B[0minformation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_load_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    269\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    270\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\mstek\\pycharmprojects\\pytorch-vq-vae\\venv\\lib\\site-packages\\gensim\\downloader.py\u001B[0m in \u001B[0;36m_load_info\u001B[1;34m(url, encoding)\u001B[0m\n\u001B[0;32m    197\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 199\u001B[1;33m         \u001B[0minfo_bytes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0murlopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    200\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mOSError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    201\u001B[0m         \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[0;32m    220\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    221\u001B[0m         \u001B[0mopener\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_opener\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 222\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mopener\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    223\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0minstall_opener\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopener\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[0;32m    523\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    524\u001B[0m         \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maudit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'urllib.Request'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfull_url\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_method\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 525\u001B[1;33m         \u001B[0mresponse\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    526\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    527\u001B[0m         \u001B[1;31m# post-process response\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36m_open\u001B[1;34m(self, req, data)\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    541\u001B[0m         \u001B[0mprotocol\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 542\u001B[1;33m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001B[0m\u001B[0;32m    543\u001B[0m                                   '_open', req)\n\u001B[0;32m    544\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36m_call_chain\u001B[1;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[0;32m    500\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mhandler\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mhandlers\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    501\u001B[0m             \u001B[0mfunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeth_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 502\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    503\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    504\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mhttps_open\u001B[1;34m(self, req)\u001B[0m\n\u001B[0;32m   1395\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1396\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mhttps_open\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1397\u001B[1;33m             return self.do_open(http.client.HTTPSConnection, req,\n\u001B[0m\u001B[0;32m   1398\u001B[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001B[0;32m   1399\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\urllib\\request.py\u001B[0m in \u001B[0;36mdo_open\u001B[1;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[0;32m   1352\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1353\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1354\u001B[1;33m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001B[0m\u001B[0;32m   1355\u001B[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001B[0;32m   1356\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;31m# timeout error\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1250\u001B[0m                 encode_chunked=False):\n\u001B[0;32m   1251\u001B[0m         \u001B[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1252\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_send_request\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencode_chunked\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1253\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1254\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_send_request\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencode_chunked\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36m_send_request\u001B[1;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[0;32m   1296\u001B[0m             \u001B[1;31m# default charset of iso-8859-1.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1297\u001B[0m             \u001B[0mbody\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_encode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'body'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1298\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mendheaders\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbody\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencode_chunked\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencode_chunked\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1299\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1300\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mgetresponse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36mendheaders\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1245\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1246\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mCannotSendHeader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1247\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_send_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage_body\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencode_chunked\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencode_chunked\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1248\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1249\u001B[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36m_send_output\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1005\u001B[0m         \u001B[0mmsg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mb\"\\r\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1006\u001B[0m         \u001B[1;32mdel\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1007\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1008\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1009\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmessage_body\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    945\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msock\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    946\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_open\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 947\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    948\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    949\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mNotConnected\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1412\u001B[0m             \u001B[1;34m\"Connect to a host on a given (SSL) port.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1413\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1414\u001B[1;33m             \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1415\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1416\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tunnel_host\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    916\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    917\u001B[0m         \u001B[1;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 918\u001B[1;33m         self.sock = self._create_connection(\n\u001B[0m\u001B[0;32m    919\u001B[0m             (self.host,self.port), self.timeout, self.source_address)\n\u001B[0;32m    920\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msetsockopt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIPPROTO_TCP\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTCP_NODELAY\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\socket.py\u001B[0m in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address)\u001B[0m\n\u001B[0;32m    794\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0msource_address\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    795\u001B[0m                 \u001B[0msock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msource_address\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 796\u001B[1;33m             \u001B[0msock\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msa\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    797\u001B[0m             \u001B[1;31m# Break explicitly a reference cycle\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    798\u001B[0m             \u001B[0merr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model_name = 'glove-wiki-gigaword-50'\n",
    "wv = api.load(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WordEmbeddingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import nltk\n",
    "\n",
    "all_data = wv.vectors[:20000]\n",
    "pos = nltk.pos_tag(wv.index_to_key[:len(all_data)])\n",
    "all_words = np.array(wv.index_to_key[:len(all_data)])\n",
    "idx = np.array([i for i, v in enumerate(all_data) if pos[i][1] in ['NN','JJ','RB','VB'] and all_words[i] not in ['\"', \"n't\", '_', '-', 'u.s.', '%', 'i']])\n",
    "all_data = all_data[idx]\n",
    "all_words = all_words[idx]\n",
    "all_data_len = len(all_data)\n",
    "training_data_len = int(all_data_len * 0.8)\n",
    "training_data = WordEmbeddingDataset(all_data[:training_data_len])\n",
    "validation_data = WordEmbeddingDataset(all_data[training_data_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_variance = np.var((training_data.data - np.min(training_data.data))\n",
    "                       / (np.max(training_data.data) - np.min(training_data.data)))\n",
    "\n",
    "cosine_similarity_eps = 1e-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Quantizer Layer\n",
    "\n",
    "This layer takes a tensor to be quantized. The channel dimension will be used as the space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize.\n",
    "\n",
    "The output tensor will have the same shape as the input.\n",
    "\n",
    "As an example for a `BCHW` tensor of shape `[16, 64, 32, 32]`, we will first convert it to an `BHWC` tensor of shape `[16, 32, 32, 64]` and then reshape it into `[16384, 64]` and all `16384` vectors of size `64`  will be quantized independently. In otherwords, the channels are used as the space in which to quantize. All other dimensions will be flattened and be seen as different examples to quantize, `16384` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        logger.debug(f'self._embedding.weight.data=\\n{self._embedding.weight.data}\\n')\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.contiguous()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        repeated_input = flat_input.repeat(1, self._embedding.weight.size(0)).view(-1, self._embedding_dim)\n",
    "        repeated_embedding = self._embedding.weight.repeat(flat_input.size(0), 1).view(-1, self._embedding_dim)\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=cosine_similarity_eps)\n",
    "        sims = cos(repeated_input, repeated_embedding).view(-1, self._embedding.weight.size(0))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmax(sims, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(flat_input.shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.cosine_embedding_loss(quantized.detach(), flat_input, torch.ones(quantized.size(0)))\n",
    "        q_latent_loss = F.cosine_embedding_loss(quantized, flat_input.detach(), torch.ones(quantized.size(0)))\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = flat_input + (quantized - flat_input).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        return loss, quantized.contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also implement a slightly modified version  which will use exponential moving averages to update the embedding vectors instead of an auxillary loss. This has the advantage that the embedding updates are independent of the choice of optimizer for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
    "        super(VectorQuantizerEMA, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.normal_()\n",
    "        logger.debug(f'self._embedding.weight.data=\\n{self._embedding.weight.data}\\n')\n",
    "        self._commitment_cost = commitment_cost\n",
    "        \n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
    "        self._ema_w.data.normal_()\n",
    "        \n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.contiguous()\n",
    "\n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        repeated_input = flat_input.repeat(1, self._embedding.weight.size(0)).view(-1, self._embedding_dim)\n",
    "        repeated_embedding = self._embedding.weight.repeat(flat_input.size(0), 1).view(-1, self._embedding_dim)\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=cosine_similarity_eps)\n",
    "        sims = cos(repeated_input, repeated_embedding).view(-1, self._embedding.weight.size(0))\n",
    "\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmax(sims, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(flat_input.shape)\n",
    "        \n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "            \n",
    "            dw = torch.matmul(encodings.t(), flat_input)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            \n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.cosine_embedding_loss(quantized.detach(), flat_input, torch.ones(quantized.size(0)))\n",
    "        loss = self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight Through Estimator\n",
    "        quantized = flat_input + (quantized - flat_input).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        return loss, quantized.contiguous(), perplexity, encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder Architecture\n",
    "\n",
    "The encoder and decoder architecture is based on a ResNet and is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_hiddens):\n",
    "        super(Residual, self).__init__()\n",
    "        self._block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=num_hiddens,\n",
    "                      out_features=num_residual_hiddens),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=num_residual_hiddens,\n",
    "                      out_features=num_hiddens)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self._block(x)\n",
    "\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        self._layers = nn.ModuleList([Residual(num_hiddens, num_residual_hiddens)\n",
    "                             for _ in range(self._num_residual_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self._num_residual_layers):\n",
    "            x = self._layers[i](x)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self._lin_1 = nn.Linear(in_features=embedding_dim,\n",
    "                                out_features=num_hiddens)\n",
    "\n",
    "        self._lin_2 = nn.Linear(in_features=num_hiddens,\n",
    "                                out_features=num_hiddens)\n",
    "\n",
    "        self._lin_3 = nn.Linear(in_features=num_hiddens,\n",
    "                                out_features=num_hiddens)\n",
    "\n",
    "        self._residual_stack = ResidualStack(num_hiddens=num_hiddens,\n",
    "                                             num_residual_layers=num_residual_layers,\n",
    "                                             num_residual_hiddens=num_residual_hiddens)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self._lin_1(inputs)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._lin_2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self._lin_3(x)\n",
    "        return self._residual_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, definition_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._definition_len = definition_len\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        definition = inputs.view((-1, self._definition_len, self._embedding_dim))\n",
    "        recon = torch.mean(definition, 1)\n",
    "        return recon, definition"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train\n",
    "\n",
    "We use the hyperparameters from the author's code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_training_updates = 15000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "\n",
    "embedding_dim = 50\n",
    "num_embeddings = 100\n",
    "definition_len = 3\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "decay = 0.99\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "logger.info(f'model_name={model_name}')\n",
    "logger.info(f'batch_size={batch_size}')\n",
    "logger.info(f'num_training_updates={num_training_updates}')\n",
    "logger.info(f'num_hiddens={num_hiddens}')\n",
    "logger.info(f'num_residual_hiddens={num_residual_hiddens}')\n",
    "logger.info(f'embedding_dim={embedding_dim}')\n",
    "logger.info(f'num_embeddings={num_embeddings}')\n",
    "logger.info(f'definition_len={definition_len}')\n",
    "logger.info(f'commitment_cost={commitment_cost}')\n",
    "logger.info(f'decay={decay}')\n",
    "logger.info(f'learning_rate={learning_rate}')\n",
    "logger.info(f'cosine_similarity_eps={cosine_similarity_eps}')\n",
    "logger.info('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(training_data, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True,\n",
    "                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(validation_data,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_hiddens, definition_len, num_residual_layers, num_residual_hiddens,\n",
    "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self._encoder = Encoder(num_hiddens,\n",
    "                                num_residual_layers, \n",
    "                                num_residual_hiddens,\n",
    "                                embedding_dim)\n",
    "        self._pre_vq_lin = nn.Linear(in_features=num_hiddens,\n",
    "                                      out_features=embedding_dim * definition_len)\n",
    "        if decay > 0.0:\n",
    "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
    "                                              commitment_cost, decay)\n",
    "        else:\n",
    "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
    "                                           commitment_cost)\n",
    "        self._decoder = Decoder(embedding_dim, definition_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = self._pre_vq_lin(z)\n",
    "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
    "        x_recon, x_definition = self._decoder(quantized)\n",
    "\n",
    "        return loss, x_recon, x_definition, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_hiddens, definition_len, num_residual_layers, num_residual_hiddens,\n",
    "              num_embeddings, embedding_dim, \n",
    "              commitment_cost, decay).to(device)\n",
    "\n",
    "logger.info(f'model=\\n{model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_vq_loss = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in xrange(num_training_updates):\n",
    "    data = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    vq_loss, data_recon, data_definition, perplexity = model(data)\n",
    "    recon_error = F.cosine_embedding_loss(data_recon, data, torch.ones(data_recon.size(0))) / data_variance\n",
    "    loss = recon_error + vq_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_vq_loss.append(vq_loss.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        logger.debug('%d iterations' % (i+1))\n",
    "        logger.debug('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "        logger.debug('vq_loss: %.3f' % np.mean(train_vq_loss[-100:]))\n",
    "        logger.debug('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "        logger.debug('')\n",
    "\n",
    "# Write final result\n",
    "logger.info('%d iterations' % (i+1))\n",
    "logger.info('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "logger.info('vq_loss: %.3f' % np.mean(train_vq_loss[-100:]))\n",
    "logger.info('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "logger.info('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
    "train_res_vq_loss_smooth = savgol_filter(train_vq_loss, 201, 7)\n",
    "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16,8))\n",
    "ax = f.add_subplot(2,2,1)\n",
    "ax.plot(train_res_recon_error_smooth)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Smoothed cosine embedding loss.')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax = f.add_subplot(2,2,2)\n",
    "ax.plot(train_res_vq_loss_smooth)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Smoothed VQ loss.')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "ax = f.add_subplot(2,2,3)\n",
    "ax.plot(train_res_perplexity_smooth)\n",
    "ax.set_title('Smoothed Average codebook usage (perplexity).')\n",
    "ax.set_xlabel('iteration')\n",
    "\n",
    "plt.savefig(f'{run_dir}/loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auxilary functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 183
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine_similarity = nn.CosineSimilarity(dim=0, eps=cosine_similarity_eps)\n",
    "\n",
    "def closest_vector(v):\n",
    "    i = np.argmax([cosine_similarity(v, torch.tensor(w).detach()) for w in all_data])\n",
    "    return all_data[i]\n",
    "\n",
    "def closest_word(v):\n",
    "    i = np.argmax([cosine_similarity(v, torch.tensor(w).detach()) for w in all_data])\n",
    "    return all_words[i]\n",
    "\n",
    "def tensor_to_closest_word(t):\n",
    "    return closest_word(t)\n",
    "\n",
    "def tensor_to_embedding_closest_word(v):\n",
    "    i = np.argmax([cosine_similarity(v, w) for w in model._vq_vae._embedding.weight.detach()])\n",
    "    return tensor_to_closest_word(model._vq_vae._embedding.weight.detach()[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "valid_originals = next(iter(validation_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "vq_output_eval = model._pre_vq_lin(model._encoder(valid_originals))\n",
    "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "valid_reconstructions, valid_definition = model._decoder(valid_quantize)\n",
    "\n",
    "for i in range(10):\n",
    "    orig_word = tensor_to_closest_word(valid_originals[i].detach())\n",
    "    recon_word = tensor_to_closest_word(valid_reconstructions[i].detach())\n",
    "    def_text = ' '.join([tensor_to_embedding_closest_word(v.detach()) for v in valid_definition[i]])\n",
    "    logger.info(f'{orig_word} to {recon_word}: {def_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_originals = next(iter(training_loader))\n",
    "train_originals = train_originals.to(device)\n",
    "\n",
    "vq_output_eval = model._pre_vq_lin(model._encoder(train_originals))\n",
    "_, train_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "train_reconstructions, train_definition = model._decoder(train_quantize)\n",
    "\n",
    "for i in range(10):\n",
    "    orig_word = tensor_to_closest_word(train_originals[i].detach())\n",
    "    recon_word = tensor_to_closest_word(train_reconstructions[i].detach())\n",
    "    def_text = ' '.join([tensor_to_embedding_closest_word(v.detach()) for v in train_definition[i]])\n",
    "    logger.info(f'{orig_word} to {recon_word}: {def_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## View Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = umap.UMAP(n_neighbors=3,\n",
    "                 min_dist=0.1,\n",
    "                 metric='cosine').fit_transform(model._vq_vae._embedding.weight.data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)\n",
    "plt.savefig(f'{run_dir}/embedding.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(num_embeddings):\n",
    "    logger.info(f'{i}:\\t{tensor_to_closest_word(model._vq_vae._embedding.weight.data[i])} {model._vq_vae._embedding.weight.data[i].tolist()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}